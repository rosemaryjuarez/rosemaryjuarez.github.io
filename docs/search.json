[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am currently a MEDS student at the Bren School of Environmental Science & Management. I focus on spatial data visualization. I\nAs an undergrad at UCSB, I was a part of the Promise Scholar Program, and awarded the Betty Wells Promise Scholar scholarship. The promise scholar is awarded to high-achieving, first-generation students from low-income housing. Students are guaranteed $120,000 across all four years, allowing for me to prioritize my education.\nFor graduate school, I also received two fellowships. The Promise Fellowship, issued by the Graduate Division at UC Santa Barbara and donated by Dr. Nien-Tsu Shen, is a total of $40,000 and is expected to cover the cost of tuition and fees for academic year 2023–24, including summer. Awarded as part of the Promise Scholar Program. The Environmental Data Science Fellowship, issued by the Bren School,recognizes outstanding academic record and will cover the cost of a professional degree.\nI have several indoor hobbies. I spend the majority of my time reading and reviewing manga, and adopting and training pet rats.\n\n\n\n\n\n\n\n Bolillo (left) and concha (right)\n Bolillo destroying my couch after leaving her alone (right)"
  },
  {
    "objectID": "about.html#section",
    "href": "about.html#section",
    "title": "About Me",
    "section": "",
    "text": "I am currently a MEDS student at the Bren School of Environmental Science & Management. I focus on spatial data visualization. I\nAs an undergrad at UCSB, I was a part of the Promise Scholar Program, and awarded the Betty Wells Promise Scholar scholarship. The promise scholar is awarded to high-achieving, first-generation students from low-income housing. Students are guaranteed $120,000 across all four years, allowing for me to prioritize my education.\nFor graduate school, I also received two fellowships. The Promise Fellowship, issued by the Graduate Division at UC Santa Barbara and donated by Dr. Nien-Tsu Shen, is a total of $40,000 and is expected to cover the cost of tuition and fees for academic year 2023–24, including summer. Awarded as part of the Promise Scholar Program. The Environmental Data Science Fellowship, issued by the Bren School,recognizes outstanding academic record and will cover the cost of a professional degree.\nI have several indoor hobbies. I spend the majority of my time reading and reviewing manga, and adopting and training pet rats.\n\n\n\n\n\n\n\n Bolillo (left) and concha (right)\n Bolillo destroying my couch after leaving her alone (right)"
  },
  {
    "objectID": "blog/2023-12-13-220-hw/hw4-task-3(1).html",
    "href": "blog/2023-12-13-220-hw/hw4-task-3(1).html",
    "title": "Air Quality Index of Santa Barbara County during the 2017 Thomas Fire",
    "section": "",
    "text": "Author: Rosemary Juarez\nLink to github repo: github repo"
  },
  {
    "objectID": "blog/2023-12-13-220-hw/hw4-task-3(1).html#purpose",
    "href": "blog/2023-12-13-220-hw/hw4-task-3(1).html#purpose",
    "title": "Air Quality Index of Santa Barbara County during the 2017 Thomas Fire",
    "section": "Purpose",
    "text": "Purpose\nIn December of 2017, The Thomas fire burned throughout Santa Barbara and Ventura county, creating distress and destructing in its wake. This wildfire was one of multiple wildfires occuring in California, making this a hotter topic throughout the years. As a native Ventura county resident, I have seen the effects this wildfire has caused in the community. Not only did I have to miss school, but my parents and I were forced to shelter indoors as the air quality grew worse. With the sky burning blood red for a few days, and smoke hazes filling up the streets of Oxnard, my curiosity grew more. Now that I have a chance to explore what exactly was going due to having data science experience, I can now vizualize the Thomas’s fire impact through air quality. My interest in collecting the air quality during this time can shed light on the correlation between wildfires and air quality. I want to investigate how the Thomas Fire impacted the AQI in Santa Barbara county using public data provided by US Environmental Protection Agency."
  },
  {
    "objectID": "blog/2023-12-13-220-hw/hw4-task-3(1).html#highlights",
    "href": "blog/2023-12-13-220-hw/hw4-task-3(1).html#highlights",
    "title": "Air Quality Index of Santa Barbara County during the 2017 Thomas Fire",
    "section": "Highlights:",
    "text": "Highlights:\n\nvizualize raster data\nTime series analysis\ngeospatial analysis\nremote sensing analysis\nFalse Color Imagery"
  },
  {
    "objectID": "blog/2023-12-13-220-hw/hw4-task-3(1).html#dataset-description",
    "href": "blog/2023-12-13-220-hw/hw4-task-3(1).html#dataset-description",
    "title": "Air Quality Index of Santa Barbara County during the 2017 Thomas Fire",
    "section": "Dataset Description",
    "text": "Dataset Description\n\nLandsat 8 imagery:\nFor raster data, this originated from NASA’s Earth Observatory. The landsat imagery has a collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite. the landsat imagery used in this report is post-2017\nThomas Fire Perimeter:\nFor our Vector data, the Thomas fire perimeter polygon came from the CA State Geoportal, which comes with multiple fire perimeters.\nAir Quality Index\nThe Air Quality Index (AQI) data from the US Environmental Protection Agency. This data provides AQI coverage period from 2017 - 2018.\n\n\nReferences to datasets\nLandsat 8 imagery: https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2\nThomas Fire Perimeter: https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about\nAQI: https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI"
  },
  {
    "objectID": "blog/2023-12-13-220-hw/hw4-task-3(1).html#citations",
    "href": "blog/2023-12-13-220-hw/hw4-task-3(1).html#citations",
    "title": "Air Quality Index of Santa Barbara County during the 2017 Thomas Fire",
    "section": "Citations",
    "text": "Citations\nUS Environmental Protection Agency. Air Quality System Data Mart. available via https://www.epa.gov/outdoor-air-quality-data. Accessed November 28, 2023.\nCALFIRE, USDA Forest Service, other Federal Partners. California Fire Perimeters(All). Available via https://gis.data.ca.gov/maps/CALFIRE-Forestry::california-fire-perimeters-all/about. Accessed November 28, 2023.\nNASA, USGS, Microsoft. Landsat Collection 2 Level-2 Science products. Available via https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2. Accessed November 28, 2023."
  },
  {
    "objectID": "blog/2023-12-14-bird-analysis/index.html",
    "href": "blog/2023-12-14-bird-analysis/index.html",
    "title": "Can proxy data help determine th Urban Heat Index in California?",
    "section": "",
    "text": "Urban heat islands (UHIs) are areas within cities that experience significantly higher temperatures compared to their rural surroundings. This phenomenon arises due to the replacement of natural surfaces with impervious materials, such as asphalt and concrete, which absorb and retain heat. UHIs pose substantial risks to human health, particularly those in dense, concrete jungles. Higher temperatures can worsen heat-related illnesses, increase cardiovascular illness risk, and contribute to dehydration. Vulnerable populations, such as the elderly and individuals with pre-existing health conditions, are particularly at risk.\nUHI are also tied to environmental concerns, as the intensified heat can also compromise air quality by promoting the formation of ground-level ozone and other pollutants. As heat increases in the city, so does energy consumption. This leads to higher energy-consumption, leading to environmental degradation.\n(heat_index.jpg)\nWe can usually spot out on a map where these heat islands occur, known as an Urban heat index map. However mapping out the index itself is a complicated task. Not only does this require satellite data for surface temperature, we require hourly air temperature. This is both time-consuming and costly. However, a question I have is whether we can use proxy data to get a closer estimate of an urban heat index, using correlation and linear regression to see if this is possible. I also want to investigate whether bird data could provide more accuracy if we applied interpolation on areas where birds cannot be accurately counted.\nI am interested in the correlation between bird count and temperature, as from personal experience, I do not hear birds often during very hot days. Finding out if it is due to the temperature is my interest.\n\n\n\nCan I draw conclusions about the urban heat islands using samples of air temperature and bird sightings?\n\n\n\nThe data was obtained from two sources: Kaggle and Ebirds.\n\nEbirds 2014: For Bird count, I used the Ebird Taxonomy List That lists from 2012-2023 all bird species count. The CSV file contains spatial coordinates, as well as z coordinates, indicating the height in which the birds were found. For Bird Species I will be focusing\nAir temperature in San diego: I received this air temperature From the website Kaggle. The data originates from the weather station located in San Diego, California. The weather station is equipped with sensors that capture weather-related measurements such as air temperature, air pressure, and relative humidity As California air temperature data is difficult to find for free, finding a cvs of daily temperature readings has been helpful. Columns I used for this is air_temp, which records the temperature every minute from 2011 to 2014.\n\nTo begin, I state my null and alternative hypothesis:\n\n\n\n\n\nI then call in all the libraries I will be using for this.\n\n\nCode\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.3.2\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(readr)\nlibrary(gt)\nlibrary(tufte)\nlibrary(feasts)\n\n\nWarning: package 'feasts' was built under R version 4.3.2\n\n\nLoading required package: fabletools\n\n\nWarning: package 'fabletools' was built under R version 4.3.2\n\n\nCode\nlibrary(here)\n\n\nhere() starts at C:/Users/rosem/Documents/MEDS/Courses/rosemaryjuarez.github.io\n\n\nCode\nlibrary(sf)\n\n\nWarning: package 'sf' was built under R version 4.3.2\n\n\nLinking to GEOS 3.11.2, GDAL 3.7.2, PROJ 9.3.0; sf_use_s2() is TRUE\n\n\nCode\nlibrary(raster)\n\n\nWarning: package 'raster' was built under R version 4.3.2\n\n\nLoading required package: sp\n\n\nWarning: package 'sp' was built under R version 4.3.2\n\n\n\nAttaching package: 'raster'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nCode\nlibrary(gstat)\n\n\nWarning: package 'gstat' was built under R version 4.3.2\n\n\nCode\nlibrary(automap)\n\n\nWarning: package 'automap' was built under R version 4.3.2\n\n\nCode\nlibrary(patchwork)\n\n\nWarning: package 'patchwork' was built under R version 4.3.2\n\n\n\nAttaching package: 'patchwork'\n\nThe following object is masked from 'package:raster':\n\n    area\n\n\nCode\nlibrary(viridis)\n\n\nWarning: package 'viridis' was built under R version 4.3.2\n\n\nLoading required package: viridisLite\n\n\nCode\nlibrary(basemaps)\n\n\nWarning: package 'basemaps' was built under R version 4.3.2\n\n\nCode\n#setting up my basemap\nset_defaults(map_service = \"osm\", map_type = \"topographic\")\n#to call the basemap\ndata(ext)\n\n\nI clean up the data by wrangling and renaming several outputs. And I end up with one data frame that has both air temperature and California Quail Count.\n\n\nCode\n#Reading in the necessary data\ncalqua &lt;- read_csv('C:/Users/rosem/Documents/MEDS/Courses/EDS-222/final_project/data/2014_calqua_bird_taxonomy.csv')\n\n\nRows: 180 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (8): LOC_ID, SUBNATIONAL1_CODE, ENTRY_TECHNIQUE, SUB_ID, OBS_ID, PROJ_...\ndbl  (14): LATITUDE, LONGITUDE, Month, Day, Year, HOW_MANY, VALID, REVIEWED,...\nlgl   (2): alt_full_spp_code, PLUS_CODE\ndate  (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nair_temp &lt;- read_csv(\"C:/Users/rosem/Documents/MEDS/Courses/EDS-222/final_project/data/San_Diego_Daily_Weather_Data_2014.csv\")\n\n\nRows: 1587257 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (12): rowID, air_pressure, air_temp, avg_wind_direction, avg_wind_speed...\ndttm  (1): hpwren_timestamp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n#setting the dates to by type date.\nair_temp$hpwren_timestamp &lt;- as.Date(air_temp$hpwren_timestamp) \ncalqua$Date &lt;- as.Date(calqua$Date) \n\n#removing any duplicates in air temperature or timestamps, and only keeping originals\nair_temp &lt;- distinct(air_temp, hpwren_timestamp, .keep_all = TRUE)\n\n#merging california Quails and air_temperature by Date\ncalqua_merge &lt;- merge(calqua, air_temp, by.x = 'Date', by.y = 'hpwren_timestamp', all.y = FALSE)\n\n\nAfter cleaning my data, I now plot a scatter plot showing the correlation between air_temperature and bird count in San Diego.\n\n\nCode\nggplot(calqua_merge, aes(x = air_temp, y = HOW_MANY)) +\n  geom_point(size = 3, color = 'navy')+\n  geom_smooth(method='lm', color ='red') +\n  xlab('Air Temperature (F)') +\n  ylab('Bird Count') +\n  labs(title = 'California Quail sightings in California')+\n  theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nI also want to run a linear regression between Air Temperature and bird count.\n\n\nCode\nsummary(lm(HOW_MANY ~ air_temp, data = calqua_merge))\n\n\n\nCall:\nlm(formula = HOW_MANY ~ air_temp, data = calqua_merge)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-28.41 -23.75 -11.84  10.11  80.54 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 24.24032   18.41337   1.316    0.191\nair_temp     0.07857    0.32915   0.239    0.812\n\nResidual standard error: 29.5 on 117 degrees of freedom\nMultiple R-squared:  0.0004868, Adjusted R-squared:  -0.008056 \nF-statistic: 0.05699 on 1 and 117 DF,  p-value: 0.8117\n\n\n\n\nWhen Looking at the summary table for our linear regression model, we find:\n\nAt zero degrees, we expect bird count total to be around 24\nFor every Fahrenheit degree increase, we expect to see .7 less bird\nOur adjusted R-squared is -0.008, meaning that there are other factors that contribute to bird sightings, but air temperature does not play a huge role in San Diego\nOur calculated p value is 0.8, which is higher than our significance of 0.05, so we cannot reject the null and accept that there is no significant linear relationship between California Quail count and air temperature\n\n\n\n\n\nafter running a linear regression analysis, we can say that we see do not see any correlation or significant linear relationship between air temperature and bird count. Meaning that either there are likely other factors that contribute to that, or there are no factors that change bird counts.\nWhile we did not find any correlation between air temperature and bird count, we could run another analysis that explores potential spots where bird count was miscounted due to poor view in trees.\nFor that we can run a kriging analysis, where we can run use interpolation to predict and estimate the locations of hidden birds.\n\n\n\n\n\nto start off, we can continue using our calqua_merge dataframe, as it has both x, y, and z coordinates. Our x and y represent longitude and latitude, and our z represents the estimated height in which the bird was recorded.\n\n\nCode\n#rename our z value to a simpler term\ncalqua_merge &lt;- calqua_merge %&gt;% \n  mutate(depth = SNOW_DEP_ATLEAST)\n\n#clean and map our bird sighting locations\ncalqua_merge %&gt;% \n  ggplot(\n  mapping = aes(x = LATITUDE, y = LONGITUDE, color = depth)) +\n  geom_point(size = 3) + \n  scale_color_viridis(option = \"B\") +\n  theme_classic() #optional \n\n\n\n\n\n\n\n\n\n\n\nWe now want to convert our dataframe to an sf class for our Variogram.\n\n\nCode\n#turning our calqua_merge into object class sf\ncalqua_sf &lt;- st_as_sf(calqua_merge, coords = c(\"LATITUDE\", \"LONGITUDE\"), crs = 25832) %&gt;% \n  cbind(st_coordinates(.))\n\n#remove na rows in calqua\nnon_na_rows &lt;- !is.na(calqua_sf$depth)\ncalqua_sf &lt;- calqua_sf[non_na_rows, ]\n\n#remove columns that have all na in calqua_sf\ncalqua_sf &lt;- calqua_sf[ , colSums(is.na(calqua_sf))==0]\n\n#remove all duplicates from calqua_sf geometry\ncalqua_sf &lt;- distinct(calqua_sf, geometry, .keep_all = TRUE)\n\n#remove na rows in calqua\nnon_na_rows &lt;- !is.na(calqua_sf$depth)\ncalqua_sf &lt;- calqua_sf[non_na_rows, ]\n\n#remove columns that have all na in calqua_sf\ncalqua_sf &lt;- calqua_sf[ , colSums(is.na(calqua_sf))==0]\n\n#remove all duplicates from calqua_sf geometry\ncalqua_sf &lt;- distinct(calqua_sf, geometry, .keep_all = TRUE)\n\n\nAfter cleaning up the data, we can now prepare and run a variogram plot.\n\n\nCode\n#creating the autofit variogram\nv_mod_full &lt;- automap::autofitVariogram(formula = X~1, calqua_sf)\n\nv_mod_calqua &lt;- v_mod_full$var_model\nhead(v_mod_calqua)\n\n\n  model        psill    range kappa\n1   Nug   0.08737953  0.00000     0\n2   Ste 827.46574208 56.95514    10\n\n\nCode\n#my fitted Variogram Model\n\nplot(v_mod_full)\n\n\n\n\n\n\n\nWe now want to plot our grid that will hold in our kriging map, which is raster. I do 400 pixels, as I want a detailed map showing potential bird sighting\n\n\nCode\ngrd_sf &lt;- st_make_grid(calqua_sf, n = c(20,20))\nplot(calqua_sf$geometry)\nplot(grd_sf, add = TRUE)\n\n\n\n\n\nWe can now run our kriging analysis, which will be stored under krig_cal.\n\n\nCode\nkrig_cal &lt;- krige(\n  depth~1, \n  calqua_sf,\n  grd_sf,\n  model = v_mod_calqua\n)\n\n\n[using ordinary kriging]\n\n\nAnd now we can plot our original map and our kriging map.\n\n\nCode\n# Plot the raw wells using geom_point()\np_raw = calqua_merge %&gt;% \n  ggplot(\n  mapping = aes(x = LATITUDE, y = LONGITUDE, color = depth)) +\n  geom_point(size = 3) + \n  scale_color_viridis(option = \"B\") +\n  theme_classic() +\n  ggtitle(label = \"calqua Sampled\") +\n  theme_void() +\n    theme(\n      plot.title = element_text(hjust = 0.5))\n  \n\n# Plot the kriging output using geom_raster()\np_kriging &lt;-p_kriging &lt;- ggplot() +\n  geom_sf(data = krig_cal, aes(fill = var1.pred)) +\n  ggtitle(label = \"Ordinary Kriging\") +\n  scale_fill_gradientn(colors = terrain.colors(100)) +\n  theme_void() +\n  theme(\n      plot.title = element_text(hjust = 0.5)\n    ) + guides(fill=guide_legend(title=\"Z\", reverse=TRUE))\nprint(p_raw) + print(p_kriging)"
  },
  {
    "objectID": "blog/2023-12-14-bird-analysis/index.html#motivation",
    "href": "blog/2023-12-14-bird-analysis/index.html#motivation",
    "title": "Can proxy data help determine th Urban Heat Index in California?",
    "section": "",
    "text": "Urban heat islands (UHIs) are areas within cities that experience significantly higher temperatures compared to their rural surroundings. This phenomenon arises due to the replacement of natural surfaces with impervious materials, such as asphalt and concrete, which absorb and retain heat. UHIs pose substantial risks to human health, particularly those in dense, concrete jungles. Higher temperatures can worsen heat-related illnesses, increase cardiovascular illness risk, and contribute to dehydration. Vulnerable populations, such as the elderly and individuals with pre-existing health conditions, are particularly at risk.\nUHI are also tied to environmental concerns, as the intensified heat can also compromise air quality by promoting the formation of ground-level ozone and other pollutants. As heat increases in the city, so does energy consumption. This leads to higher energy-consumption, leading to environmental degradation.\n(heat_index.jpg)\nWe can usually spot out on a map where these heat islands occur, known as an Urban heat index map. However mapping out the index itself is a complicated task. Not only does this require satellite data for surface temperature, we require hourly air temperature. This is both time-consuming and costly. However, a question I have is whether we can use proxy data to get a closer estimate of an urban heat index, using correlation and linear regression to see if this is possible. I also want to investigate whether bird data could provide more accuracy if we applied interpolation on areas where birds cannot be accurately counted.\nI am interested in the correlation between bird count and temperature, as from personal experience, I do not hear birds often during very hot days. Finding out if it is due to the temperature is my interest."
  },
  {
    "objectID": "blog/2023-12-14-bird-analysis/index.html#question",
    "href": "blog/2023-12-14-bird-analysis/index.html#question",
    "title": "Can proxy data help determine th Urban Heat Index in California?",
    "section": "",
    "text": "Can I draw conclusions about the urban heat islands using samples of air temperature and bird sightings?"
  },
  {
    "objectID": "blog/2023-12-14-bird-analysis/index.html#data-and-methods",
    "href": "blog/2023-12-14-bird-analysis/index.html#data-and-methods",
    "title": "Can proxy data help determine th Urban Heat Index in California?",
    "section": "",
    "text": "The data was obtained from two sources: Kaggle and Ebirds.\n\nEbirds 2014: For Bird count, I used the Ebird Taxonomy List That lists from 2012-2023 all bird species count. The CSV file contains spatial coordinates, as well as z coordinates, indicating the height in which the birds were found. For Bird Species I will be focusing\nAir temperature in San diego: I received this air temperature From the website Kaggle. The data originates from the weather station located in San Diego, California. The weather station is equipped with sensors that capture weather-related measurements such as air temperature, air pressure, and relative humidity As California air temperature data is difficult to find for free, finding a cvs of daily temperature readings has been helpful. Columns I used for this is air_temp, which records the temperature every minute from 2011 to 2014.\n\nTo begin, I state my null and alternative hypothesis:\n\n\n\n\n\nI then call in all the libraries I will be using for this.\n\n\nCode\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.3.2\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(readr)\nlibrary(gt)\nlibrary(tufte)\nlibrary(feasts)\n\n\nWarning: package 'feasts' was built under R version 4.3.2\n\n\nLoading required package: fabletools\n\n\nWarning: package 'fabletools' was built under R version 4.3.2\n\n\nCode\nlibrary(here)\n\n\nhere() starts at C:/Users/rosem/Documents/MEDS/Courses/rosemaryjuarez.github.io\n\n\nCode\nlibrary(sf)\n\n\nWarning: package 'sf' was built under R version 4.3.2\n\n\nLinking to GEOS 3.11.2, GDAL 3.7.2, PROJ 9.3.0; sf_use_s2() is TRUE\n\n\nCode\nlibrary(raster)\n\n\nWarning: package 'raster' was built under R version 4.3.2\n\n\nLoading required package: sp\n\n\nWarning: package 'sp' was built under R version 4.3.2\n\n\n\nAttaching package: 'raster'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nCode\nlibrary(gstat)\n\n\nWarning: package 'gstat' was built under R version 4.3.2\n\n\nCode\nlibrary(automap)\n\n\nWarning: package 'automap' was built under R version 4.3.2\n\n\nCode\nlibrary(patchwork)\n\n\nWarning: package 'patchwork' was built under R version 4.3.2\n\n\n\nAttaching package: 'patchwork'\n\nThe following object is masked from 'package:raster':\n\n    area\n\n\nCode\nlibrary(viridis)\n\n\nWarning: package 'viridis' was built under R version 4.3.2\n\n\nLoading required package: viridisLite\n\n\nCode\nlibrary(basemaps)\n\n\nWarning: package 'basemaps' was built under R version 4.3.2\n\n\nCode\n#setting up my basemap\nset_defaults(map_service = \"osm\", map_type = \"topographic\")\n#to call the basemap\ndata(ext)\n\n\nI clean up the data by wrangling and renaming several outputs. And I end up with one data frame that has both air temperature and California Quail Count.\n\n\nCode\n#Reading in the necessary data\ncalqua &lt;- read_csv('C:/Users/rosem/Documents/MEDS/Courses/EDS-222/final_project/data/2014_calqua_bird_taxonomy.csv')\n\n\nRows: 180 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (8): LOC_ID, SUBNATIONAL1_CODE, ENTRY_TECHNIQUE, SUB_ID, OBS_ID, PROJ_...\ndbl  (14): LATITUDE, LONGITUDE, Month, Day, Year, HOW_MANY, VALID, REVIEWED,...\nlgl   (2): alt_full_spp_code, PLUS_CODE\ndate  (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nair_temp &lt;- read_csv(\"C:/Users/rosem/Documents/MEDS/Courses/EDS-222/final_project/data/San_Diego_Daily_Weather_Data_2014.csv\")\n\n\nRows: 1587257 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (12): rowID, air_pressure, air_temp, avg_wind_direction, avg_wind_speed...\ndttm  (1): hpwren_timestamp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n#setting the dates to by type date.\nair_temp$hpwren_timestamp &lt;- as.Date(air_temp$hpwren_timestamp) \ncalqua$Date &lt;- as.Date(calqua$Date) \n\n#removing any duplicates in air temperature or timestamps, and only keeping originals\nair_temp &lt;- distinct(air_temp, hpwren_timestamp, .keep_all = TRUE)\n\n#merging california Quails and air_temperature by Date\ncalqua_merge &lt;- merge(calqua, air_temp, by.x = 'Date', by.y = 'hpwren_timestamp', all.y = FALSE)\n\n\nAfter cleaning my data, I now plot a scatter plot showing the correlation between air_temperature and bird count in San Diego.\n\n\nCode\nggplot(calqua_merge, aes(x = air_temp, y = HOW_MANY)) +\n  geom_point(size = 3, color = 'navy')+\n  geom_smooth(method='lm', color ='red') +\n  xlab('Air Temperature (F)') +\n  ylab('Bird Count') +\n  labs(title = 'California Quail sightings in California')+\n  theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "blog/2023-12-14-bird-analysis/index.html#analysis",
    "href": "blog/2023-12-14-bird-analysis/index.html#analysis",
    "title": "Can proxy data help determine th Urban Heat Index in California?",
    "section": "",
    "text": "I also want to run a linear regression between Air Temperature and bird count.\n\n\nCode\nsummary(lm(HOW_MANY ~ air_temp, data = calqua_merge))\n\n\n\nCall:\nlm(formula = HOW_MANY ~ air_temp, data = calqua_merge)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-28.41 -23.75 -11.84  10.11  80.54 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 24.24032   18.41337   1.316    0.191\nair_temp     0.07857    0.32915   0.239    0.812\n\nResidual standard error: 29.5 on 117 degrees of freedom\nMultiple R-squared:  0.0004868, Adjusted R-squared:  -0.008056 \nF-statistic: 0.05699 on 1 and 117 DF,  p-value: 0.8117\n\n\n\n\nWhen Looking at the summary table for our linear regression model, we find:\n\nAt zero degrees, we expect bird count total to be around 24\nFor every Fahrenheit degree increase, we expect to see .7 less bird\nOur adjusted R-squared is -0.008, meaning that there are other factors that contribute to bird sightings, but air temperature does not play a huge role in San Diego\nOur calculated p value is 0.8, which is higher than our significance of 0.05, so we cannot reject the null and accept that there is no significant linear relationship between California Quail count and air temperature"
  },
  {
    "objectID": "blog/2023-12-14-bird-analysis/index.html#conclusion",
    "href": "blog/2023-12-14-bird-analysis/index.html#conclusion",
    "title": "Can proxy data help determine th Urban Heat Index in California?",
    "section": "",
    "text": "after running a linear regression analysis, we can say that we see do not see any correlation or significant linear relationship between air temperature and bird count. Meaning that either there are likely other factors that contribute to that, or there are no factors that change bird counts.\nWhile we did not find any correlation between air temperature and bird count, we could run another analysis that explores potential spots where bird count was miscounted due to poor view in trees.\nFor that we can run a kriging analysis, where we can run use interpolation to predict and estimate the locations of hidden birds."
  },
  {
    "objectID": "blog/2023-12-14-bird-analysis/index.html#part-two-kriging-analysis",
    "href": "blog/2023-12-14-bird-analysis/index.html#part-two-kriging-analysis",
    "title": "Can proxy data help determine th Urban Heat Index in California?",
    "section": "",
    "text": "to start off, we can continue using our calqua_merge dataframe, as it has both x, y, and z coordinates. Our x and y represent longitude and latitude, and our z represents the estimated height in which the bird was recorded.\n\n\nCode\n#rename our z value to a simpler term\ncalqua_merge &lt;- calqua_merge %&gt;% \n  mutate(depth = SNOW_DEP_ATLEAST)\n\n#clean and map our bird sighting locations\ncalqua_merge %&gt;% \n  ggplot(\n  mapping = aes(x = LATITUDE, y = LONGITUDE, color = depth)) +\n  geom_point(size = 3) + \n  scale_color_viridis(option = \"B\") +\n  theme_classic() #optional \n\n\n\n\n\n\n\n\n\n\n\nWe now want to convert our dataframe to an sf class for our Variogram.\n\n\nCode\n#turning our calqua_merge into object class sf\ncalqua_sf &lt;- st_as_sf(calqua_merge, coords = c(\"LATITUDE\", \"LONGITUDE\"), crs = 25832) %&gt;% \n  cbind(st_coordinates(.))\n\n#remove na rows in calqua\nnon_na_rows &lt;- !is.na(calqua_sf$depth)\ncalqua_sf &lt;- calqua_sf[non_na_rows, ]\n\n#remove columns that have all na in calqua_sf\ncalqua_sf &lt;- calqua_sf[ , colSums(is.na(calqua_sf))==0]\n\n#remove all duplicates from calqua_sf geometry\ncalqua_sf &lt;- distinct(calqua_sf, geometry, .keep_all = TRUE)\n\n#remove na rows in calqua\nnon_na_rows &lt;- !is.na(calqua_sf$depth)\ncalqua_sf &lt;- calqua_sf[non_na_rows, ]\n\n#remove columns that have all na in calqua_sf\ncalqua_sf &lt;- calqua_sf[ , colSums(is.na(calqua_sf))==0]\n\n#remove all duplicates from calqua_sf geometry\ncalqua_sf &lt;- distinct(calqua_sf, geometry, .keep_all = TRUE)\n\n\nAfter cleaning up the data, we can now prepare and run a variogram plot.\n\n\nCode\n#creating the autofit variogram\nv_mod_full &lt;- automap::autofitVariogram(formula = X~1, calqua_sf)\n\nv_mod_calqua &lt;- v_mod_full$var_model\nhead(v_mod_calqua)\n\n\n  model        psill    range kappa\n1   Nug   0.08737953  0.00000     0\n2   Ste 827.46574208 56.95514    10\n\n\nCode\n#my fitted Variogram Model\n\nplot(v_mod_full)\n\n\n\n\n\n\n\nWe now want to plot our grid that will hold in our kriging map, which is raster. I do 400 pixels, as I want a detailed map showing potential bird sighting\n\n\nCode\ngrd_sf &lt;- st_make_grid(calqua_sf, n = c(20,20))\nplot(calqua_sf$geometry)\nplot(grd_sf, add = TRUE)\n\n\n\n\n\nWe can now run our kriging analysis, which will be stored under krig_cal.\n\n\nCode\nkrig_cal &lt;- krige(\n  depth~1, \n  calqua_sf,\n  grd_sf,\n  model = v_mod_calqua\n)\n\n\n[using ordinary kriging]\n\n\nAnd now we can plot our original map and our kriging map.\n\n\nCode\n# Plot the raw wells using geom_point()\np_raw = calqua_merge %&gt;% \n  ggplot(\n  mapping = aes(x = LATITUDE, y = LONGITUDE, color = depth)) +\n  geom_point(size = 3) + \n  scale_color_viridis(option = \"B\") +\n  theme_classic() +\n  ggtitle(label = \"calqua Sampled\") +\n  theme_void() +\n    theme(\n      plot.title = element_text(hjust = 0.5))\n  \n\n# Plot the kriging output using geom_raster()\np_kriging &lt;-p_kriging &lt;- ggplot() +\n  geom_sf(data = krig_cal, aes(fill = var1.pred)) +\n  ggtitle(label = \"Ordinary Kriging\") +\n  scale_fill_gradientn(colors = terrain.colors(100)) +\n  theme_void() +\n  theme(\n      plot.title = element_text(hjust = 0.5)\n    ) + guides(fill=guide_legend(title=\"Z\", reverse=TRUE))\nprint(p_raw) + print(p_kriging)"
  },
  {
    "objectID": "blog/redlining-analysis/index.html",
    "href": "blog/redlining-analysis/index.html",
    "title": "Redline analysis in LA",
    "section": "",
    "text": "In the 1930s, the Home Owners’ Loan Corporation (HOLC), a New Deal initiative, assessed neighborhoods for real estate investment safety. The HOLC’s grading system (A for green, B for blue, C for yellow, and D for red) was then used to deny loans for home ownership, a discriminatory practice known as “redlining.” This historical injustice has affected community wealth and health. Redlined areas exhibit lower greenery levels and higher temperatures compared to other neighborhoods.\nI want to examine how this is still a current-day issue by outlining the socioeconomics of current tracts. A recent study found that redlining has not only affected the environments communities are exposed to, it has also shaped our observations of biodiversity.Ellis-Soto and co-authors found that redlined neighborhoods remain the most undersampled areas across 195 US cities. This gap is highly concerning, because conservation decisions are made based on these data[4].\nCheck out coverage by EOS.\n\n\n\n\nWe will be working with data from the United States Environmental Protection Agency’s EJScreen: Environmental Justice Screening and Mapping Tool. EJScreen provides on environmental and demographic information for the US at the Census tract and block group levels.\n\n\n\nI will be working with maps of HOLC grade designations for Los Angeles. Information on the data can be found here.1\n\n\n\nThe Global Biodiversity Information Facility is the largest aggregator of biodiversity observations in the world. Observations typically include a location and date that a species was observed.\nWe will be working observations of birds from 2021 onward.\n\n\n\n\n\n\n\nLoad relevant packages.\n\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(tmap)\nlibrary(tmap)    # for static and interactive maps\nlibrary(leaflet) # for interactive maps\nlibrary(ggplot2) # tidyverse data visualization package\nlibrary(here)\n\nI will first Read in EJScreen data and filter to Los Angeles County\n\n#reading ejscreen here\nejscreen &lt;- st_read(\"C:/Users/rosem//Documents/MEDS/Courses/EDS-223/assignment-2-rosemaryjuarez/data/EJSCREEN_2023_BG_StatePct_with_AS_CNMI_GU_VI.gdb\")\n\nReading layer `EJSCREEN_StatePctiles_with_AS_CNMI_GU_VI' from data source \n  `C:\\Users\\rosem\\Documents\\MEDS\\Courses\\EDS-223\\assignment-2-rosemaryjuarez\\data\\EJSCREEN_2023_BG_StatePct_with_AS_CNMI_GU_VI.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 243021 features and 223 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -19951910 ymin: -1617130 xmax: 16259830 ymax: 11554350\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n\nI will then need to do some data wrangling, by filtering to only the county and census block groups that I am interested with. I am interested in viewing the 95th percentile of national valies for wastewater discharge by adding a centroid.\n\n#filtering to los angeles only\nlos_angeles &lt;- filter(ejscreen, CNTY_NAME == \"Los Angeles County\" )\n\n#got to filter first:\nwater_95 &lt;- filter(los_angeles, P_PWDIS &gt;95) %&gt;% \n    st_centroid()\n\n#fixing to make it zoom in closer without islands\nlos_angeles &lt;- los_angeles %&gt;% \n  filter(P_PWDIS != 'na')\n\ntm_shape(los_angeles) +\n  tm_polygons(fill = 'P_PWDIS',\n              title = 'Percentile of wastewater discharge') +\n  tm_shape(water_95) +\n  tm_dots('P_PWDIS',\n          fill = 'red') +\n  tm_scale_bar() +\n  tm_compass() +\n  tm_title('Los Angeles Census block: above 95th percentile in Wastewater Discharge')\n\n\n\n#yay first map down!\n\nI willnot filter and look for census block groups that have less than 5% of the population considered low-income, find the 80th percentile for particulate matter 2.5, and those above 80th percentile for superfund proximity. We are looking at several groups as we are interested in comparing those in affluent and poor areas\n\n#first setting low_income\nlow_income &lt;- los_angeles %&gt;% \n  filter(LOWINCPCT &lt; .05)\n\n#calculating the percentage of census block groups\n(nrow(low_income)/nrow(los_angeles)) * 100\n\n[1] 5.895795\n\n#found 80th percentile and above in the same line\npm2_5 &lt;- los_angeles %&gt;% \n  filter((P_PM25 &gt; 80) & (P_PNPL &gt; 80))\n\nNow that I have filtered for factors I am interested in, I will now begin to investigate redlining by downloading the geojson of the redlining in Los Angeles.\n\nLA_redlining &lt;- st_read(\"https://dsl.richmond.edu/panorama/redlining/static/citiesData/CALosAngeles1939/geojson.json\") %&gt;%\n  st_make_valid()\n\nReading layer `geojson' from data source \n  `https://dsl.richmond.edu/panorama/redlining/static/citiesData/CALosAngeles1939/geojson.json' \n  using driver `GeoJSON'\nSimple feature collection with 417 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -118.6104 ymin: 33.70563 xmax: -117.7028 ymax: 34.30388\nGeodetic CRS:  WGS 84\n\n\nNow i will show a map of historical redlining boundaries, colored by HOLC grade. A reminder that the HOLC grading system are 4 bins: A for green, B for blue, C for yellow, and D for red\n\n#first need to read in data and see what it looks like\ntm_shape(LA_redlining) +\n  tm_polygons(fill = 'fill') +\n  tm_compass() +\n  tm_scale_bar() +\n  tm_graticules() +\n  tm_title('HOLC Grade in Los Angeles')\n\n\n\n\nI now what to find the number of census block groups that fall within areas with HOLC. Finally I want to summarize all the current conditions from the EJSCREEN data. I will find the mean for those in low income, low life expectancy, and air toxics cancer risk\n\n#change CRS\nLA_redlining &lt;- st_transform(LA_redlining, \"EPSG:3857\")\n\n#find census block groups using join\n\nholc_census &lt;- st_join(x = los_angeles, y = LA_redlining)\n\n#crs change again\nholc_census &lt;- st_transform(holc_census,\"EPSG:3857\" )\n\nholc_summary &lt;- holc_census %&gt;% \n  group_by(fill) %&gt;% \n  summarise(lowinc_mean = mean(LOWINCPCT, na.rm = TRUE) * 100,\n            pm_25_mean = mean(P_PM25, na.rm = TRUE),\n            life_exp_mean = mean(P_LIFEEXPPCT, na.rm = TRUE),\n            cancer_mean = mean(P_CANCER, na.rm = TRUE)\n            )\n\nholc_summary\n\nFrom the evidence above, Class D consistently has the worst ratings throughout the table. It not only has the lowest average on Socio-Economic Status, but it also suffers from higher risk of cancer or lower life expectancy. Meanwhile class A has consistent top marks in health and wealth.\n\n\n\nFor bird observations from 2022 that fall within neighborhoods with HOLC grads, I find the percent of observations within each redlining categories and plot results.\n\n#read in bird data:\nbirds &lt;- st_read(\"C:/Users/rosem/Documents/MEDS/Courses/EDS-223/assignment-2-rosemaryjuarez/data/gbif-birds-LA/gbif-birds-LA.shp\")\n\nReading layer `gbif-birds-LA' from data source \n  `C:\\Users\\rosem\\Documents\\MEDS\\Courses\\EDS-223\\assignment-2-rosemaryjuarez\\data\\gbif-birds-LA\\gbif-birds-LA.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1288865 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -118.6099 ymin: 33.70563 xmax: -117.7028 ymax: 34.30385\nGeodetic CRS:  WGS 84\n\n\n\nbirds &lt;- birds %&gt;% \n  filter(year == 2022)\n\nRight now i am redefining the crs for birds, and joining both birds and holc census data. Finally, now I can finally filter and analyze the summary between birds and HOLC grade. I would assume the more afluent neighborhoods would have higher biodiversity.\n\n#change CRS\nbirds &lt;- st_transform(birds, \"EPSG:3857\")\n\n\n# bird_holc &lt;- st_join(x = holc_census, y = birds)\n\n\n#summary_birds_holc &lt;- bird_holc %&gt;% \n  #group_by(grade) %&gt;% \n  #summarize(count = n()) %&gt;% \n # mutate(percentage = (count/sum(count)) *100) %&gt;% \n  #st_drop_geometry() %&gt;% \n # na.omit()\n\n\n#ggplot(data = summary_birds_holc, aes(x = grade, y = percentage))+\n # geom_bar(stat = 'identity', fill = 'green', col = 'darkgreen')+\n  #theme_minimal()+\n  #labs(x = 'HOLC Grade', y = 'Percentage of Birds Found', title = 'Bird Observations within HOLC Grades')\n\n\n\n\nbirds_holc\n\n\n\n\n\n\nI think these are somewhat surprising, as i did not expect to see HOLC Grade a to be lower than b or C. As Mentioned previously, I would assume those in more affluent neighborhoods would have more biodiversity. To show that B has more biodiversity must mean that there are some factors that O did not consider"
  },
  {
    "objectID": "blog/redlining-analysis/index.html#overview",
    "href": "blog/redlining-analysis/index.html#overview",
    "title": "Redline analysis in LA",
    "section": "",
    "text": "In the 1930s, the Home Owners’ Loan Corporation (HOLC), a New Deal initiative, assessed neighborhoods for real estate investment safety. The HOLC’s grading system (A for green, B for blue, C for yellow, and D for red) was then used to deny loans for home ownership, a discriminatory practice known as “redlining.” This historical injustice has affected community wealth and health. Redlined areas exhibit lower greenery levels and higher temperatures compared to other neighborhoods.\nI want to examine how this is still a current-day issue by outlining the socioeconomics of current tracts. A recent study found that redlining has not only affected the environments communities are exposed to, it has also shaped our observations of biodiversity.Ellis-Soto and co-authors found that redlined neighborhoods remain the most undersampled areas across 195 US cities. This gap is highly concerning, because conservation decisions are made based on these data[4].\nCheck out coverage by EOS.\n\n\n\n\nWe will be working with data from the United States Environmental Protection Agency’s EJScreen: Environmental Justice Screening and Mapping Tool. EJScreen provides on environmental and demographic information for the US at the Census tract and block group levels.\n\n\n\nI will be working with maps of HOLC grade designations for Los Angeles. Information on the data can be found here.1\n\n\n\nThe Global Biodiversity Information Facility is the largest aggregator of biodiversity observations in the world. Observations typically include a location and date that a species was observed.\nWe will be working observations of birds from 2021 onward."
  },
  {
    "objectID": "blog/redlining-analysis/index.html#assignment",
    "href": "blog/redlining-analysis/index.html#assignment",
    "title": "Redline analysis in LA",
    "section": "",
    "text": "Load relevant packages.\n\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(tmap)\nlibrary(tmap)    # for static and interactive maps\nlibrary(leaflet) # for interactive maps\nlibrary(ggplot2) # tidyverse data visualization package\nlibrary(here)\n\nI will first Read in EJScreen data and filter to Los Angeles County\n\n#reading ejscreen here\nejscreen &lt;- st_read(\"C:/Users/rosem//Documents/MEDS/Courses/EDS-223/assignment-2-rosemaryjuarez/data/EJSCREEN_2023_BG_StatePct_with_AS_CNMI_GU_VI.gdb\")\n\nReading layer `EJSCREEN_StatePctiles_with_AS_CNMI_GU_VI' from data source \n  `C:\\Users\\rosem\\Documents\\MEDS\\Courses\\EDS-223\\assignment-2-rosemaryjuarez\\data\\EJSCREEN_2023_BG_StatePct_with_AS_CNMI_GU_VI.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 243021 features and 223 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -19951910 ymin: -1617130 xmax: 16259830 ymax: 11554350\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n\nI will then need to do some data wrangling, by filtering to only the county and census block groups that I am interested with. I am interested in viewing the 95th percentile of national valies for wastewater discharge by adding a centroid.\n\n#filtering to los angeles only\nlos_angeles &lt;- filter(ejscreen, CNTY_NAME == \"Los Angeles County\" )\n\n#got to filter first:\nwater_95 &lt;- filter(los_angeles, P_PWDIS &gt;95) %&gt;% \n    st_centroid()\n\n#fixing to make it zoom in closer without islands\nlos_angeles &lt;- los_angeles %&gt;% \n  filter(P_PWDIS != 'na')\n\ntm_shape(los_angeles) +\n  tm_polygons(fill = 'P_PWDIS',\n              title = 'Percentile of wastewater discharge') +\n  tm_shape(water_95) +\n  tm_dots('P_PWDIS',\n          fill = 'red') +\n  tm_scale_bar() +\n  tm_compass() +\n  tm_title('Los Angeles Census block: above 95th percentile in Wastewater Discharge')\n\n\n\n#yay first map down!\n\nI willnot filter and look for census block groups that have less than 5% of the population considered low-income, find the 80th percentile for particulate matter 2.5, and those above 80th percentile for superfund proximity. We are looking at several groups as we are interested in comparing those in affluent and poor areas\n\n#first setting low_income\nlow_income &lt;- los_angeles %&gt;% \n  filter(LOWINCPCT &lt; .05)\n\n#calculating the percentage of census block groups\n(nrow(low_income)/nrow(los_angeles)) * 100\n\n[1] 5.895795\n\n#found 80th percentile and above in the same line\npm2_5 &lt;- los_angeles %&gt;% \n  filter((P_PM25 &gt; 80) & (P_PNPL &gt; 80))\n\nNow that I have filtered for factors I am interested in, I will now begin to investigate redlining by downloading the geojson of the redlining in Los Angeles.\n\nLA_redlining &lt;- st_read(\"https://dsl.richmond.edu/panorama/redlining/static/citiesData/CALosAngeles1939/geojson.json\") %&gt;%\n  st_make_valid()\n\nReading layer `geojson' from data source \n  `https://dsl.richmond.edu/panorama/redlining/static/citiesData/CALosAngeles1939/geojson.json' \n  using driver `GeoJSON'\nSimple feature collection with 417 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -118.6104 ymin: 33.70563 xmax: -117.7028 ymax: 34.30388\nGeodetic CRS:  WGS 84\n\n\nNow i will show a map of historical redlining boundaries, colored by HOLC grade. A reminder that the HOLC grading system are 4 bins: A for green, B for blue, C for yellow, and D for red\n\n#first need to read in data and see what it looks like\ntm_shape(LA_redlining) +\n  tm_polygons(fill = 'fill') +\n  tm_compass() +\n  tm_scale_bar() +\n  tm_graticules() +\n  tm_title('HOLC Grade in Los Angeles')\n\n\n\n\nI now what to find the number of census block groups that fall within areas with HOLC. Finally I want to summarize all the current conditions from the EJSCREEN data. I will find the mean for those in low income, low life expectancy, and air toxics cancer risk\n\n#change CRS\nLA_redlining &lt;- st_transform(LA_redlining, \"EPSG:3857\")\n\n#find census block groups using join\n\nholc_census &lt;- st_join(x = los_angeles, y = LA_redlining)\n\n#crs change again\nholc_census &lt;- st_transform(holc_census,\"EPSG:3857\" )\n\nholc_summary &lt;- holc_census %&gt;% \n  group_by(fill) %&gt;% \n  summarise(lowinc_mean = mean(LOWINCPCT, na.rm = TRUE) * 100,\n            pm_25_mean = mean(P_PM25, na.rm = TRUE),\n            life_exp_mean = mean(P_LIFEEXPPCT, na.rm = TRUE),\n            cancer_mean = mean(P_CANCER, na.rm = TRUE)\n            )\n\nholc_summary\n\nFrom the evidence above, Class D consistently has the worst ratings throughout the table. It not only has the lowest average on Socio-Economic Status, but it also suffers from higher risk of cancer or lower life expectancy. Meanwhile class A has consistent top marks in health and wealth.\n\n\n\nFor bird observations from 2022 that fall within neighborhoods with HOLC grads, I find the percent of observations within each redlining categories and plot results.\n\n#read in bird data:\nbirds &lt;- st_read(\"C:/Users/rosem/Documents/MEDS/Courses/EDS-223/assignment-2-rosemaryjuarez/data/gbif-birds-LA/gbif-birds-LA.shp\")\n\nReading layer `gbif-birds-LA' from data source \n  `C:\\Users\\rosem\\Documents\\MEDS\\Courses\\EDS-223\\assignment-2-rosemaryjuarez\\data\\gbif-birds-LA\\gbif-birds-LA.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1288865 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -118.6099 ymin: 33.70563 xmax: -117.7028 ymax: 34.30385\nGeodetic CRS:  WGS 84\n\n\n\nbirds &lt;- birds %&gt;% \n  filter(year == 2022)\n\nRight now i am redefining the crs for birds, and joining both birds and holc census data. Finally, now I can finally filter and analyze the summary between birds and HOLC grade. I would assume the more afluent neighborhoods would have higher biodiversity.\n\n#change CRS\nbirds &lt;- st_transform(birds, \"EPSG:3857\")\n\n\n# bird_holc &lt;- st_join(x = holc_census, y = birds)\n\n\n#summary_birds_holc &lt;- bird_holc %&gt;% \n  #group_by(grade) %&gt;% \n  #summarize(count = n()) %&gt;% \n # mutate(percentage = (count/sum(count)) *100) %&gt;% \n  #st_drop_geometry() %&gt;% \n # na.omit()\n\n\n#ggplot(data = summary_birds_holc, aes(x = grade, y = percentage))+\n # geom_bar(stat = 'identity', fill = 'green', col = 'darkgreen')+\n  #theme_minimal()+\n  #labs(x = 'HOLC Grade', y = 'Percentage of Birds Found', title = 'Bird Observations within HOLC Grades')\n\n\n\n\nbirds_holc"
  },
  {
    "objectID": "blog/redlining-analysis/index.html#conclusion",
    "href": "blog/redlining-analysis/index.html#conclusion",
    "title": "Redline analysis in LA",
    "section": "",
    "text": "I think these are somewhat surprising, as i did not expect to see HOLC Grade a to be lower than b or C. As Mentioned previously, I would assume those in more affluent neighborhoods would have more biodiversity. To show that B has more biodiversity must mean that there are some factors that O did not consider"
  },
  {
    "objectID": "blog/redlining-analysis/index.html#footnotes",
    "href": "blog/redlining-analysis/index.html#footnotes",
    "title": "Redline analysis in LA",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRobert K. Nelson, LaDale Winling, Richard Marciano, Nathan Connolly, et al., “Mapping Inequality,” American Panorama, ed. Robert K. Nelson and Edward L. Ayers, accessed October 17, 2023, https://dsl.richmond.edu/panorama/redlining/↩︎"
  },
  {
    "objectID": "blog_settings.html",
    "href": "blog_settings.html",
    "title": "blog",
    "section": "",
    "text": "this is the start of my first blog post\n\n\n\n\n\n\n\n\n\n\n\n\nAir Quality Index of Santa Barbara County during the 2017 Thomas Fire\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan proxy data help determine th Urban Heat Index in California?\n\n\nUsing proxy data to determine if they are accurate enough to use as an alternate to Urban heat index\n\n\n\nRosemary Juarez\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization and Infographic Design Elements\n\n\n\n\n\n\nRosemary Juarez\n\n\nMar 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRedline analysis in LA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "books/2024-02-08-first-post-junko/index.html",
    "href": "books/2024-02-08-first-post-junko/index.html",
    "title": "Junko - Love him, Not Me!",
    "section": "",
    "text": "CitationBibTeX citation:@online{juarez2024,\n  author = {Juarez, Rosemary},\n  title = {Junko - {Love} Him, {Not} {Me!}},\n  date = {2024-02-08},\n  url = {https://rosemaryjuarez.github.io/books/2024-02-08-first-post-junko/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJuarez, Rosemary. 2024. “Junko - Love Him, Not Me!”\nFebruary 8, 2024. https://rosemaryjuarez.github.io/books/2024-02-08-first-post-junko/."
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Book Reviews",
    "section": "",
    "text": "A brief list of reviews on my current collection. As of now, my collection has around ~800 volumes. With more than 150 unique titles. it is a continuous growing collection. To find out more about what manga in general is, click here, from the British Museum.\n.Japanese mangas are organized by demographics rather than genre. The demographics are composed of 4 categories, which include: shonen(young boy), seinen(adult male), shoujo(young girl), and Josei(adult women). These japanese demographics all have vastly ranging genres within each demographic.\nI specifically read shoujo and Josei mangas, as not only am I within the demographic, in enjoy the focus on character development and introspection that i notice more than in other categories. As someone who enjoys reading romance, this demographic range also includes multiple endless stories on this.\nI will post reviews accordingly here on my book review posts. posts will likely not be in reading order unless i explicitly mention."
  },
  {
    "objectID": "books.html#book-reviews-on-my-manga-collection",
    "href": "books.html#book-reviews-on-my-manga-collection",
    "title": "Book Reviews",
    "section": "",
    "text": "A brief list of reviews on my current collection. As of now, my collection has around ~800 volumes. With more than 150 unique titles. it is a continuous growing collection. To find out more about what manga in general is, click here, from the British Museum.\n.Japanese mangas are organized by demographics rather than genre. The demographics are composed of 4 categories, which include: shonen(young boy), seinen(adult male), shoujo(young girl), and Josei(adult women). These japanese demographics all have vastly ranging genres within each demographic.\nI specifically read shoujo and Josei mangas, as not only am I within the demographic, in enjoy the focus on character development and introspection that i notice more than in other categories. As someone who enjoys reading romance, this demographic range also includes multiple endless stories on this.\nI will post reviews accordingly here on my book review posts. posts will likely not be in reading order unless i explicitly mention."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rosemary Juarez",
    "section": "",
    "text": "Rosemary is a working towards a Master of Environmental Data Science (Expected June 2024)"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Rosemary Juarez",
    "section": "Education",
    "text": "Education\nUniversity of California, Santa Barbara | San Barbara, CA B.A in Geography, GIS Emphasis | Sept 2019 - June 2023"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "cool resources!",
    "section": "",
    "text": "theres nothing so far, but its okay"
  },
  {
    "objectID": "blog/2024-03-10-crime-infographic/index.html",
    "href": "blog/2024-03-10-crime-infographic/index.html",
    "title": "Data Visualization and Infographic Design Elements",
    "section": "",
    "text": "Figure 1. Dangers of Los Angeles. Three main takeaways: Latinos are the top victims of crime reports, bodily forces are the top recorded “weapons” used, and hand guns are the most common firearms to encounter.\n\n\n\n\nThis infographic on Los Angeles city crimes from 2020-2024 was created using R and Procreate. This has been a 9 week-long project that has helped me develop more practice with data visualization using R. Three main elements were considered for this project:\n\ntheme\ncontextualizing my data\ntext adjustments within R\n\nWhile there are many other elements that went into this project, those three main ideas has really fueled the project as a whole.\n\n\n\nI explored on Los Angeles crime report data. This data interest me due to the darker theme surrounding this dataset. The dataset ranges from relatively benign crimes such as robbery to darker situations such as homicides. Knowing that this dataset consists of darker themes, I wanted to reflect on that accordingly. Knowing that some of the subjects are on the more delicate side, I instead wanted to focus on the general idea of the dataset: what can we take away from it? And from the data exploration I conducted, I have been most intrigued by who the victims are and will most likely deal with in the event of a crime.\n\n\n\nAs mentioned previously,from the data exploration I conducted, I have been most intrigued by who the victims are and will most likely deal with in the event of a crime. To put the project into context, it was somewhat simple, as I was largely interested in counts. My main goal was to find the count of each variable, and find what are the most common crimes, victims, locations, ect. And after compiling several graphs, I decided to focus on the more general ideas such as top weapons, firearms, and victims.\n\n\n\nThe most time-consuming yet eye-opening part of this project is adjusting and creating this graph in R. Not only that, but I have also learned the importance of reproducible data and the standards of tidy data. One element from ggplot2 that I have improved on significantly is learning how theme() and labs work well together. I used to prefer python more when it came to data visualization, but after taking this course, I realized that I was wrong and R is actually a great place to make graphs!\n\n\n\n\ngraphic form: I have explored other graphic forms using this dataset, such as a radar plot! however I realized that line charts are slightly better at visualizing trends when it comes to interpretation (and time).\ntypography: I had fun figuring out the typefont and sizes I would like for my infographic. It took multiple trial and errors to figure out the right font and size.\ngeneral design: I realized that my idea of using a detective board as my background could be considered a bit too busy or distracting of my data. To mitigate that, I darkened my plot, and highlighted my main plots using lighting.\ncolor: I created a color palette when creating my infographic. However due to some time constraints, my graph is not exactly ready for printing, as there are still some more cohesive color schemes to consider (and color blind friendly options too for my older 2 brothers). However I believe that by highlighting the graphs and darkening the rest of the background, it helps with the colors being a bit more cojesive. ## Process\n\nFor the libraries I used for my infogrpahic:\n\n\nShow the code\n# setting my chunk options\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n\n#list of packages\nlibrary(tidyverse) #main use for data wrangling\nlibrary(janitor) #helps with clean names for my variables\nlibrary(lubridate) #need this for my time data. Mostly for wrangling time data\nlibrary(stringr) #this helps with dealing with strings and characters in my data\nlibrary(showtext) #choosing fonts from google fonts\n\n\nfont_add_google(name = \"Special Elite\") #for the typewriter font\nfont_add_google(name = \"Nosifer\") #for the bloody font\n\nshowtext_auto() #to render text\n\n\n\n\nShow the code\n#reading in my data from my local computer\nla_crimes &lt;- read_csv(\"C:/Users/rosem/Documents/MEDS/Courses/EDS-240/HW/Juarez-eds240-HW4/data/Crime_Data_from_2020_to_Present_20240131.csv\") %&gt;% \n  clean_names()"
  },
  {
    "objectID": "blog/2024-03-10-crime-infographic/index.html#infographic-image",
    "href": "blog/2024-03-10-crime-infographic/index.html#infographic-image",
    "title": "Data Visualization and Infographic Design Elements",
    "section": "",
    "text": "Figure 1. Dangers of Los Angeles. Three main takeaways: Latinos are the top victims of crime reports, bodily forces are the top recorded “weapons” used, and hand guns are the most common firearms to encounter.\n\n\n\n\nThis infographic on Los Angeles city crimes from 2020-2024 was created using R and Procreate. This has been a 9 week-long project that has helped me develop more practice with data visualization using R. Three main elements were considered for this project:\n\ntheme\ncontextualizing my data\ntext adjustments within R\n\nWhile there are many other elements that went into this project, those three main ideas has really fueled the project as a whole.\n\n\n\nI explored on Los Angeles crime report data. This data interest me due to the darker theme surrounding this dataset. The dataset ranges from relatively benign crimes such as robbery to darker situations such as homicides. Knowing that this dataset consists of darker themes, I wanted to reflect on that accordingly. Knowing that some of the subjects are on the more delicate side, I instead wanted to focus on the general idea of the dataset: what can we take away from it? And from the data exploration I conducted, I have been most intrigued by who the victims are and will most likely deal with in the event of a crime.\n\n\n\nAs mentioned previously,from the data exploration I conducted, I have been most intrigued by who the victims are and will most likely deal with in the event of a crime. To put the project into context, it was somewhat simple, as I was largely interested in counts. My main goal was to find the count of each variable, and find what are the most common crimes, victims, locations, ect. And after compiling several graphs, I decided to focus on the more general ideas such as top weapons, firearms, and victims.\n\n\n\nThe most time-consuming yet eye-opening part of this project is adjusting and creating this graph in R. Not only that, but I have also learned the importance of reproducible data and the standards of tidy data. One element from ggplot2 that I have improved on significantly is learning how theme() and labs work well together. I used to prefer python more when it came to data visualization, but after taking this course, I realized that I was wrong and R is actually a great place to make graphs!\n\n\n\n\ngraphic form: I have explored other graphic forms using this dataset, such as a radar plot! however I realized that line charts are slightly better at visualizing trends when it comes to interpretation (and time).\ntypography: I had fun figuring out the typefont and sizes I would like for my infographic. It took multiple trial and errors to figure out the right font and size.\ngeneral design: I realized that my idea of using a detective board as my background could be considered a bit too busy or distracting of my data. To mitigate that, I darkened my plot, and highlighted my main plots using lighting.\ncolor: I created a color palette when creating my infographic. However due to some time constraints, my graph is not exactly ready for printing, as there are still some more cohesive color schemes to consider (and color blind friendly options too for my older 2 brothers). However I believe that by highlighting the graphs and darkening the rest of the background, it helps with the colors being a bit more cojesive. ## Process\n\nFor the libraries I used for my infogrpahic:\n\n\nShow the code\n# setting my chunk options\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n\n#list of packages\nlibrary(tidyverse) #main use for data wrangling\nlibrary(janitor) #helps with clean names for my variables\nlibrary(lubridate) #need this for my time data. Mostly for wrangling time data\nlibrary(stringr) #this helps with dealing with strings and characters in my data\nlibrary(showtext) #choosing fonts from google fonts\n\n\nfont_add_google(name = \"Special Elite\") #for the typewriter font\nfont_add_google(name = \"Nosifer\") #for the bloody font\n\nshowtext_auto() #to render text\n\n\n\n\nShow the code\n#reading in my data from my local computer\nla_crimes &lt;- read_csv(\"C:/Users/rosem/Documents/MEDS/Courses/EDS-240/HW/Juarez-eds240-HW4/data/Crime_Data_from_2020_to_Present_20240131.csv\") %&gt;% \n  clean_names()"
  },
  {
    "objectID": "blog/2024-03-10-crime-infographic/index.html#data-wrangling-and-processing",
    "href": "blog/2024-03-10-crime-infographic/index.html#data-wrangling-and-processing",
    "title": "Data Visualization and Infographic Design Elements",
    "section": "Data Wrangling and Processing",
    "text": "Data Wrangling and Processing\n\n\nShow the code\n#==============================================================\n#                 data wrangling\n# =============================================================\n\n#will create a new column that describes the main 5 race categories. for reference:\n#c(B - Black C - Chinese D - Cambodian F - Filipino G - Guamanian H - Hispanic/Latin/Mexican I - American Indian/Alaskan Native J - Japanese K - Korean L - Laotian O - Other P - Pacific Islander S - Samoan U - Hawaiian V - Vietnamese W - White X - Unknown Z - Asian Indian)\n\nasian_countries &lt;-  c('A', 'C', 'D', 'F',\"L\", 'J', 'K', 'V', 'Z')\n\n\n# Define a named vector mapping the current categories to their full names\nrace_names &lt;- c(\n  \"A\" = \"Asian\",\n  \"B\" = \"Black\",\n  \"W\" = \"White\",\n  \"H\" = \"Hispanic\",\n  \"I\" = \"Native American/Alaska\",\n  \"P\" = \"pacific Islander\"\n)\n\n#------------------------\n#   regualar wrangling\n# -----------------------\n\n#creating a cleaned-up version of la_crimes. keeping the name so that i have less names to remember\n\nla_crimes &lt;- la_crimes %&gt;% \n  #removing zeros in the `vict_age` column, as 0, -1, and -2 indicated that no age was recorded.\n  filter(vict_age &gt; 0 ) %&gt;% \n  #I want to incllude all values for victim sex, however to first test out my plots, i want to view just male and female for simplicity\n  #\n  #\n  filter(vict_sex %in% c('M', 'F')) %&gt;% \n  #Asian countries will be agreggated to one.\n  \n\n\n#-----------------------------\n#   asian country aggregation\n#-----------------------------\n\n  #Asian countries will be aggregated to one. using case_when as it will help with selecting and reassigning asian countries to the letter \"A\" if the list of values i provided above are within asian_countries\n  mutate(race_category = case_when(\n    vict_descent %in% asian_countries ~ \"A\",\n    TRUE ~ vict_descent  # Keep non-Asian races unchanged, as true will allow for the row that do not have an asian country to remain the same within the new \"race_category\" column.\n  )) %&gt;% \n\n  #filter for the top 6 race categories\n  filter(race_category %in% c('B', 'H', 'W', 'I', 'P', 'A')) %&gt;% \n\n# Rename the categories in the race column\n  mutate(race = case_match(race_category, \"B\" ~ \"Black\",\n                           \"H\" ~ \"Hispanic\",\n                           \"W\"~ \"White\",\n                           \"I\" ~ \"Native American/Alaska\",\n                           \"P\" ~ \"Pacific Islanders\",\n                           \"A\" ~ \"asian\"\n                           ))\n\n\n#====================================\n#     new data frames for plotting\n#====================================\n\n#creating crime description\ncrime_desc &lt;- la_crimes %&gt;% \n  group_by(crm_cd_desc) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange((desc(count)), .by_group = TRUE) %&gt;% \n  ungroup()\n\n\n#crime description by sex\ncrime_desc_sex &lt;- la_crimes %&gt;% \n  group_by(crm_cd_desc, vict_sex) %&gt;%\n  summarise(count = n()) %&gt;% \n  slice_max(order_by = count, n = 10) %&gt;% \n  group_by(crm_cd_desc) %&gt;% \n  ungroup()\n\n#creating weapon description\nweap_desc &lt;- la_crimes %&gt;% \n  group_by(weapon_desc) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange((desc(count)), .by_group = TRUE) %&gt;% \n  na.omit() %&gt;% \n\n#filtering those that are unknown or not physical\n\n  filter(!grepl(\"UNKNOWN\", weapon_desc)) %&gt;% \n  filter(!grepl(\"OTHER\", weapon_desc)) %&gt;% \n  filter(!grepl(\"VERBAL\", weapon_desc))\n\n#of those weapons, which ones are guns?\nweap_gun &lt;- weap_desc %&gt;% \n  filter(grepl(\"GUN\", weapon_desc) | grepl(\"PISTOL\", weapon_desc) | grepl(\"RIFLE\", weapon_desc))"
  },
  {
    "objectID": "blog/2024-03-10-crime-infographic/index.html#plotting-the-plots",
    "href": "blog/2024-03-10-crime-infographic/index.html#plotting-the-plots",
    "title": "Data Visualization and Infographic Design Elements",
    "section": "plotting the plots",
    "text": "plotting the plots\n\n\nShow the code\ntop_5_weap &lt;- weap_desc %&gt;% \n  slice(1:5)%&gt;% \n  ggplot( aes(x = fct_reorder(weapon_desc, count), y = count)) +\n  geom_col(fill = \"black\") +\n  geom_text(aes(label = count),family = \"Special Elite\", hjust = -.2, color = \"red4\", size = 9) +\n  coord_flip() +\n  theme_classic()+\n  labs(title = \"You Will Likely Encounter Bodily Forces than Anything Else\"\n       \n       ) +\n  scale_y_continuous(limits = c(0, 200000)) +\n  theme(axis.title.y = element_blank(),\n        axis.line.y = element_blank(), \n        axis.ticks.y = element_blank(),\n        axis.title.x = element_blank(),\n        axis.line.x = element_blank(), \n        axis.ticks.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_text(size = 15, family = \"Special Elite\"),\n        plot.title = element_text(size = 36, family = \"Special Elite\", face = \"bold\", color = \"red4\"),\n        plot.background = element_blank(),\n        panel.background = element_blank(),\n        \n        )\n\ntop_5_weap\n\n\n\n\n\n\n\nShow the code\nfont_add_google(name = \"Special Elite\") #for the typewriter font\nfont_add_google(name = \"Nosifer\") #for the bloody font\n\n#I want to record the top 5 crime reports involving guns\ngun_graph &lt;- weap_gun %&gt;% \n  slice(1:5)%&gt;% \n  ggplot( aes(x = fct_reorder(weapon_desc, count), y = count)) +\n  geom_col(fill = \"black\") +\n  geom_text(aes(label = count),family = \"Special Elite\", hjust = -.2, color = \"red4\", size = 9) +\n  coord_flip() +\n  theme_classic()+\n  labs(title = \"However If You Encounter Firearms, Watch Out for Handguns\"#,\n       #subtitle = \"From 2020-2024, There were 589,000 individual crime reports. From those reports, 24,000 involve firearm weapons.\"\n       ) +\n  scale_y_continuous(limits = c(0, 20000)) +\n  theme(axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.title.x = element_blank(),\n        axis.line.y = element_blank(), \n        axis.line.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.ticks.x = element_blank(),\n        plot.title = element_text(family = \"Nosifer\",\n                                  size = 35, \n                                  color = \"red4\",\n                                  hjust = 1),\n        axis.text = element_text(family = \"Special Elite\", \n                                 size = 15),\n        axis.title = element_text(family = \"Special Elite\"),\n        plot.margin = margin(1,.5,.5,.5, \"cm\"),\n        plot.background = element_blank(),\n        panel.background = element_blank()\n        \n        \n       # plot.background = element_rect(fill = \"#dbb69c\"),\n       # panel.background = element_rect(fill = \"#dbb69c\")\n        ) \ngun_graph\n\n\n\n\n\n\n\nShow the code\n# I am reporting on the top 5 race victims in the crime data\nrace_pie &lt;- la_crimes %&gt;%\n  count(race) %&gt;%\n  ggplot() +\n  ggforce::geom_arc_bar(\n    aes(x0 = 0, y0 = 0, r0 = 0.8, r = 1, amount = n, fill = race),\n   stat = \"pie\") +\n  theme_void()+\n\n  scale_fill_manual(values= c(\"#cad2c5\", \"#84a98c\", \"#52796f\", \"#354f52\", \"#0c1113\", \"#2f3e46\")) + \n  labs(title = \"Careful if you are Latino:\",\n       subtitle = \"you are the top victim of crime reports\",\n       fill = \"\") +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    panel.grid.major.y = element_blank(),\n    axis.line = element_blank(),\n    plot.background = element_blank(),\n    legend.text = element_text(size=25, family = \"Special Elite\", color = \"red4\"),\n    legend.position = c(0.55,0.52),\n    plot.title = element_text(size = 40, family = \"Special Elite\", color = \"red4\"),\n    plot.subtitle = element_text(size = 25, family = \"Special Elite\", color = \"red4\")\n    \n    )\n\nrace_pie"
  }
]